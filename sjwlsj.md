## Charpter 2 神经元模型和网络结果
1. 函数
  - (对称)硬极限函数 0(-1),1函数
  - (对称)(饱和)线性函数
  - 对数S型函数
  - 双曲正切S性函数 
  - 正线性函数 >=0
  - 竞争函数 n最大的神经元为1 其他为0

2. 模块
  - 延时模块 a(t) = u(t-1)
  - 积分器
  - 递归网络 输出部分连接到输入部分

## Charpter 3 一个说明性实例 
#### 3.2.3 Hamming网络
专门为了求解二值模式识别问题而被设计的  分为前馈层和递归(反馈)层  
前馈层 计算内积  递归层进行竞争竞争后只有一个神经元输出不为0  权值矩阵为[1,-epsion;-epsion ,1] 而 a(t+1) = poslin(W*a(t)) 即向量中每个元素都减掉另一个元素的一部分
![](http://o88xvi2w5.bkt.clouddn.com/sjwlsj3.2.3.png)

#### 3.2.4 Hopfield网络 
不断利用W和b进行迭代 最后得到标准形式
![](http://o88xvi2w5.bkt.clouddn.com/sjwlsj3.2.4.png)

## Charpter 4 感知器学习
最早提出的一种人工神经元模型  计算加权和 若加权和大于阈值 则为1 否则则为0 
增强学习: 不像有监督学习一样每一个输入提供对应的输出 而是仅仅给出一个级别 这个级别是网络对模型输入序列上的性能测度
#### 4.2.3 感知器的学习
1. 测试  计算判断
2. 构造 若正确 不变w 否则根据错误 根据已有的方向向量对应改变w  lambda * w lambda in (0,1)

#### 4.2.4 收敛性证明
只要保证线性可分 一定能在有限步内收敛

## Charpter 5 信号和权值向量空间
1. 向量空间: 一组定义在标量域上且满足一系列条件的元素集合
2. 线性无关: 存在n个标量 是sum(ai * xi)=0 且存在一个ai非0
3. 内积: 存在连续集合的内积  原本的乘变成积分
4. 范数: 基于向量长度的操作  满足以下条件
![](http://o88xvi2w5.bkt.clouddn.com/sjwlsj5.2.5.png)
5. 正交性: 若两个向量正交 当且仅当这两个向量内积为0
6. 向量展开式: 根据基向量 用一组标量配合基向量表示某个向量
7. 互逆基向量: 当向量展开时基向量不是正交的 需要用互逆基向量

## Charpter 6 神经网络中的线性变换
1. 线性变换 满足 A(x+y) = A(x)+A(y) A(kx) = k A(x)
2. 线性变换可以用矩阵乘来表示
3. 基变换 基向量得到的矩阵 可以进行坐标变换
4. 对角话 B<sup>-1</sup>AB  B是特征向量组成的矩阵

## Charpter 7 有监督的Hebb学习
#### 7.1 目的
Hebb假设: 当细胞A的轴突到细胞B的距离近到足够激励它，且反复的持续的刺激B，那么在这两个细胞或一个细胞会发送魔种增长过程或代谢反应，增加A对B的刺激效果

#### 7.2.2 Hebb规则
wij(new) = wij(old) + alpha * fi(ai) * gj(pj)  
可以看出 fi 是输入  gj 是输出 当输入和输出同号 则增加权值 异号则减少权值 与之前的Hebb假设一致  
对于有监督的来说 会用目标输出代替实际输出  
若输入的向量是标准正交向量 则能有100%的正确率  若不是则会有误差  

#### 7.2.3 仿逆规则
由于非标准正交会出现误差 故引入仿逆规则  目的是选一个矩阵 使二次误差最小  
当输入的维数比个数大时  无法得到逆矩阵 故需要仿逆
P<sup>+</sup> = (P<sup>T</sup>P)<sup>-1</sup>P<sup>T</sup>

#### 7.2.5 Hebb学习的变形
可以加入衰减项 可以更好的记忆近期的输入  W(new) = (1-lambda) \* W(old) + alpha \*t \* p  
增量规则 W(new) = W(old) + alpha \* (t-a) \*p 根据误差调整

## Charpter 8 性能曲面和最优点
- 一个正定的赫森矩阵是一个强极小点存在二阶充分条件 二阶必要条件是半正定 极小点处梯度为0是一阶必要条件
- 二次函数可以用 F(x) = 1/2 X<sup>T</sup>AX + d<sup>T</sup>X + c表示 A为对称矩阵 那么他的导数和 赫森矩阵分别为 AX+d A 根据这个 可以得到 
  1. 特征值全正 有1强极大点
  2. 特征值全负 有1强极小点
  3. 特征值有正有负 有鞍点
  4. 特征值非负且存在0 要么有一个弱极小点 或没有驻点
  5. 特征值非正且存在0 要么有一个弱极大点 或没有驻点  

以上 弱极值相当于通常的极值 强极值相当于最值

## Charpter 9 性能优化
#### 9.2.1 最速下降法
1. 稳定的学习速度  
基础是梯度下降法 在梯度下降法的基础上 对学习速度alpha 进行控制 获得稳定的速度 xk+1 = xk -alpha(Axk + d) => xk+1 = [I - alpha A]xk -alpha *d  当I-alpha A的特征值小于1 这是稳定的 =>   
alpha < 2/A的赫森矩阵的最大特征值
2. 沿直线最小化
![](http://o88xvi2w5.bkt.clouddn.com/sjwlsj9.2.1.png)

#### 9.2.2 牛顿法
牛顿法与最速下降算法不一样 是基于二阶的泰勒级数  
F(x+1) = F(x+delta) = F(xk) + g*delta + 1/2*delte *A *delta 
牛顿法直接令梯度为0  即可计算delta  
速度通常比最速下降法块 可能会振荡和发散  需要计算逆值

#### 9.2.3 共轭梯度法
共轭: 当且仅当 pk<sup>T</sup>Apj=0 k!=j  则称集合{pk}对于一个正定的赫森矩阵两两共轭  
已经确定存在一个共轭方向集{pi} 的准确
具体算法:
![](http://o88xvi2w5.bkt.clouddn.com/sjwlsj9.2.3.png)

## Charpter 10 Widrow-Hoff 学习算法
#### 10.2.1 ADALINE网路
此网络与感知器网络有基本结构 只是使用了线性传输   
#### 10.2.3LMS算法
在均方误差下 可以得到一层的ADALINE网络只存在一个最小值 故可以用之前的算法进行计算从而最小均方误差  
LMS算法: 根据最速下降法得到的 其中 I-2alphaR 的最大特征值小于1即可稳定 R = E(z*z<sup>T</sup>) z为输入向量

## Charpter 13 联想学习
#### 13.2.2 无监督的Hebb规则
在该节用了无监督的Hebb规则 在进行联想学习 利用了识别香蕉气味来识别香蕉的方法 反过来改变对香蕉气味的权值  
衰减速度: 可以对W(q-1) 乘上一个小于1的常数 使其逐渐遗忘之前的学习 防止因为噪声使参数趋于无限大

#### 13.2.3 简单的识别网络 
之前讨论的是标量输入/输出的联想 在讨论有向量输入的神经元时  叫做instar神经元  
若要活跃 则w<sup>T</sup>p = ||w|| ||p|| cos >=-b  只要设置合理的b=-||w|| ||p||  那么神经元只有在准确的w下才能活跃 这要就有一个只能识别w的神经元  
可以通过改别b的精度来得到不同精度的w

#### 13.2.4 instar规则
带衰减的Hebb规则有一个问题就是要求不停的重复,否则联想会丢失 而更好的解决方法是设置只有神经元在活跃的时候才会衰减  
instar规则: w(q) = w(q-1) +alpha *ai*pj - gama*ai*w(q-1) 有时候设置gama=alpha来简化  
kohonen规则 w(q) = w(q-1) + alpha(p(q)-w(q-1))  
以上p为输入向量 ai为输出   
instar规则适用返回0,1的函数 kohonen规则直接学习输入向量 应用与识别应用

#### 13.2.5 简单记忆网络
outstar网络有一个标量输入和一个向量输出 完成模式回忆  
输入输出表达式 a=satlins(Wp) satlins是对称饱和函数 

#### 13.2.6 outstar规则
设置衰减项和输入成正比  
w(q) = w(q-1) + alpha \*ai(q)\*pj(q)-gama\*pj(q)\*wij(q-1)

## Charpter14 竞争网络
#### 14.2.1 Hamming网络
第一层是前馈层 将输入向量和原型向量联系起来 由于一个instar只能识别一种模式 故需要多个instar  
第二层是竞争层 竞争结束后只有一个神经元有非零输出  
用第一层的输出初始化第二层 对于W 对角线上全设置成1 其他的为某个小负数  
胜者全得: 只有一个神经元有非零输出  

#### 14.2.2 竞争层
1. 竞争学习
  在竞争过后 胜利的神经元用Kogonen规则进行学习
2. 存在的问题
  1. 学习速度和稳定性的折衷
  2. 某个簇的权值向量会侵入另一权值向量的领地 破坏分类状况
  3. 存在一个神经元的初始权值向量太远 以至于从未获得竞争的胜利 解决方法 给每个神经元加上一个负的偏置量 使经常胜利的神经元胜利机会减少

#### 14.2.4 自组织特征图
SOFM网络
