## Charpter 2 神经元模型和网络结果
1. 函数
  - (对称)硬极限函数 0(-1),1函数
  - (对称)(饱和)线性函数
  - 对数S型函数
  - 双曲正切S性函数 
  - 正线性函数 >=0
  - 竞争函数 n最大的神经元为1 其他为0

2. 模块
  - 延时模块 a(t) = u(t-1)
  - 积分器
  - 递归网络 输出部分连接到输入部分

## Charpter 3 一个说明性实例 
#### 3.2.3 Hamming网络
专门为了求解二值模式识别问题而被设计的  分为前馈层和递归(反馈)层  
前馈层 计算内积  递归层进行竞争竞争后只有一个神经元输出不为0  权值矩阵为[1,-epsion;-epsion ,1] 而 a(t+1) = poslin(W*a(t)) 即向量中每个元素都减掉另一个元素的一部分
![](http://o88xvi2w5.bkt.clouddn.com/sjwlsj3.2.3.png)

#### 3.2.4 Hopfield网络 
不断利用W和b进行迭代 最后得到标准形式
![](http://o88xvi2w5.bkt.clouddn.com/sjwlsj3.2.4.png)

## Charpter 4 感知器学习
最早提出的一种人工神经元模型  计算加权和 若加权和大于阈值 则为1 否则则为0 
增强学习: 不像有监督学习一样每一个输入提供对应的输出 而是仅仅给出一个级别 这个级别是网络对模型输入序列上的性能测度
#### 4.2.3 感知器的学习
1. 测试  计算判断
2. 构造 若正确 不变w 否则根据错误 根据已有的方向向量对应改变w  lambda * w lambda in (0,1)

#### 4.2.4 收敛性证明
只要保证线性可分 一定能在有限步内收敛

## Charpter 5 信号和权值向量空间
1. 向量空间: 一组定义在标量域上且满足一系列条件的元素集合
2. 线性无关: 存在n个标量 是sum(ai * xi)=0 且存在一个ai非0
3. 内积: 存在连续集合的内积  原本的乘变成积分
4. 范数: 基于向量长度的操作  满足以下条件
![](http://o88xvi2w5.bkt.clouddn.com/sjwlsj5.2.5.png)
5. 正交性: 若两个向量正交 当且仅当这两个向量内积为0
6. 向量展开式: 根据基向量 用一组标量配合基向量表示某个向量
7. 互逆基向量: 当向量展开时基向量不是正交的 需要用互逆基向量

## Charpter 6 神经网络中的线性变换
1. 线性变换 满足 A(x+y) = A(x)+A(y) A(kx) = k A(x)
2. 线性变换可以用矩阵乘来表示
3. 基变换 基向量得到的矩阵 可以进行坐标变换
4. 对角话 B<sup>-1</sup>AB  B是特征向量组成的矩阵

## Chapter 7 有监督的Hebb学习
#### 7.1 目的
Hebb假设: 当细胞A的轴突到细胞B的距离近到足够激励它，且反复的持续的刺激B，那么在这两个细胞或一个细胞会发送魔种增长过程或代谢反应，增加A对B的刺激效果

#### 7.2.2 Hebb规则
wij(new) = wij(old) + alpha * fi(ai) * gj(pj)  
可以看出 fi 是输入  gj 是输出 当输入和输出同号 则增加权值 异号则减少权值 与之前的Hebb假设一致  
对于有监督的来说 会用目标输出代替实际输出  
若输入的向量是标准正交向量 则能有100%的正确率  若不是则会有误差  

#### 7.2.3 仿逆规则


