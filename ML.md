# Chapter 1 绪论
#### 1.2 基本术语
- 分类和回归的区别  分类是预测离散值 回归是预测连续值 
- 根据训练数据是否拥有标记信息 分为有监督和无监督  回归是前者代表 聚类是后者代表
- 通常假设每个样本都是独立同分布 iid

#### 1.3 假设空间
我们可以把学习过程看作一个在所有假设组成的空间中搜索的过程  
假设空间: 所有可能的假设组成的空间 即指所有搜索的可能 自顶向下  
版本空间: 与训练集一致的假设集合 搜索方法 

#### 1.4 归纳偏好
归纳偏好是指在选择模型的时候进行的选择  
奥卡姆剃刀法则: 若存在多个假设与观察一致 选择最简单的那一个  
没有免费的午餐定理(NFL): 在问题出现的机会相同或者所有问题同等重要下学习算法的期望性相同。   
告诉我们谈论算法必须针对问题

# Chapter 2 模型评估与选择
#### 2.1 经验误差与过拟合
- 训练误差(经验误差): 学习器在训练集上的误差  
- 泛华误差: 学习器在新样本上的误差  
- 欠拟合和过拟合 欠拟合比较容易客服 如在决策树中加扩展分支 在神经网络中增加训练轮数  过拟合无法避免 只能缓解

#### 2.2 评估方法
1. 留出法  
  将数据集分成两个互斥的集合  一个用于训练集S 一个为测试集T  
  分层采样 保证训练数据和测试数据中正例和反例比例相同  
  分隔合理 例如分隔时位置 对于某个参数  
  多次平均

2. 交叉验证法  
  将数据集分成k个大小相似的子集 每个子集尽量保证数据分布一致性 每次用k-1个子集的并进行训练 用剩下一个进程测验 那么可以测验k次 取这k次的平均  
  与此相关的 留一法 每次只取一个样本测验 只能用于样本数量不大的情况下

3. 自助法(bootstraping)
  每次从原样本随机挑选一个样本  持续m次即得到一个大小为m的样本 在m次取样中 有
  每次从原样本随机挑选一个样本  持续m次即得到一个大小为m的样本 在m次取样中 有36.8%的与样本不同 我们就可以将这m个样本做训练集 它和原样本的差集作为测试集合 

#### 2.3 性能度量
- 回归任务最常用的是均方误差   
- 查准率 precision 查全率 recall P = TP/(TP+FP)  R=TP/(TP+FN)   
  查准率和查全率通常是一对矛盾的度量  一者高时另外一个通常会低  
- PR曲线   
  若要比较两个学习器的能力 那么可以比较他们的面积   
  平衡点 BEP 当查准率=查全率的点  
- F1 和Fbeta  对查准率和查全率有不同的要求的时候用Fbeta  Fbeta=(1+beta^2)*P*R/((beta^2)*P+R)   
  当beta>1 时对查全率有更大的影响  当beta<1时对查准率有更大影响
- 若有多次训练 则可以得到一个二维混淆矩阵 那么可以得到平均的P和R 然后在利用平均的PR 得到平均的F1
- ROC和AUC  
  在对一个实数值进行预测的时候 通常会将其放在[0,1]中 如果大于某个阈值则为正例 那么就可以通过确定阈值进行确定 如果更重视差准 那么则放置较前  反之相同   
  ROC ROC曲线纵轴为TPR 真正例率 横轴为FPR 假正率  
  与PR曲线相同 面积越大则学习器越佳 面积叫AUC  
  形式化上AUC是确定预测的质量 定义loss = 1/(m<sup>+</sup>m<sup>-</sup>) (sum(is(f(x<sup>+</sup>)>f(x<sup>-</sup>)))+sum(is(f(x<sup>+</sup>)=f(x<sup>-</sup>)))/2)  AUC = 1-loss
- 非均等代价  
  由于有时候将正例判成负例和将负例判成正例说带来的代价不同，故可以将他们赋权值 赋权之后会得到新的错误率以及ROC曲线

#### 2.4 比较检验
机器学习重点比较检验大多为假设检验
- 二分检验 在机器学习中，判断错误的清空可以用二项分布模型 对应当可以用二项检验来判断在某个置信区间下 学习器的泛化错误情况
- 对同一个学习器的多次建业可以用t检验来判断泛化性能  利用t分布及计算得到的方差和均值 可以得到在一定置信区间下的泛化错误情况
- 交叉t检验 针对两个学习器 对于两个学习器 我们主要要判断两个学习器的泛化学习能力是否有显著差异 我们用两个学习器的插值的绝对值当做变量 并判断他在k-1的自由度下的t分布   
  在这里有x折的概念  由于害怕实验训练集合有一定程度的重叠故错误率不独立 故引入了x折  x折代表每x次 需要计算这x次的方差
- NcNemar检验 同样是针对两个学习器 两个学习器对同一测试样例 可能有4种结果关于是否测试正确 其中一方错误另一方正确的概率应该是相同的 在这里这两个概率的差应该服从正态分布 均值为1 那么可以设计对应的卡方分布 根据卡方分布判断是否有显著差异
- Friedman检验和Nemenyi检验   
  Friedman检验是对多个学习器的检验  将他们的测试性能进行排序 若有相同的平分序值 得到一个序值表 之后得到平均序值  
  平均序值服从正态分布 均值为(k+1)/2 方差为(k^2-1)/12 可以设置卡方分布变量  
  也可以用F分布代替卡方分布 F = (N-1)*x/(N(k-1)*x^2) x代表卡方分布  他服从k-1 和(k-1)(N-1) 的F分布  
  这可以判断所有算法是否显著相同 若有不同则可以用Nemenyi进行后续检验  
  CD = q(k*(k+1)/(6N))^0.5  若两个算法的平均序值超过CD 那么可以拒绝两个算法性能相同这个假设
#### 2.5 偏差和方差
通过公司据算可以看到泛化误差是偏差 方差 噪声的和  
偏差和方差是相互冲突的一对量  偏差-方差窘境

# Chapter 3 线性模型
#### 3.2 线性回归
从另外一个角度看待线性回归  
通过向量化 求导 后得到 w = (X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>y  
如果X<sup>T</sup>X 是满秩矩阵 那么有唯一的值 若不这户解出多个w 那么就需要引入正则化  
广义线性模型  在某个函数下进行线性回归

#### 3.3 对数几率回归
引入sigmod函数  y = 1/(1+e<sup>-z</sup>)  
主要使用了极大似然法  即求最小值 即导数为0的时候的beta 有两种方法 一种是梯度下降 一种是牛顿迭代

#### 3.4 线性判别分析 LDA
思想: 将数据投影到一条线上  同类的点应该尽可能近 异类的点应该尽可能远 那么就可以将需要预测的点投影到这条线上进行分析  
同类的要求是他们的协方差和尽可能小 异类的中心距离尽可能大  
之后就可以得到LDA的公式 J 我们所需要的也就是最大化J  
由于Sb 和Sw都为已知  且上下皆有W W<sup>T</sup>  可以将一个定成1 另外的用拉格朗日乘子法 奇异值分解求极值  
可以扩展到多个类别 

#### 3.5 多分类学习
多分类需要转变到二分类  这个可以分成3种
1. 一对一(OVO)  做n*(n-1)/2次 每两个比一次 最终看哪个多
2. 一对多(OVR)  做n次 一个为正例 其他都为反例
3. 多对多(MVM) 
常用的是ECOC纠错输出码 对N个类做M次划分 做M个分类器  之后在进行解码 根据距离判断 编码越长能更好的纠错 若长度相同 那么任意两个类别越远越好

#### 3.6 类别不平衡问题
常用三种方法   
1. 欠采样 去除部分反例 时间开销小 可能丢失信息  EasyEnsemble
2. 过采样 增加部分正例 不能直接复制正例  SMOTE
3. 阈值移动  根据训练集正反例的比例移动阈值

# Chapter 4 决策树
#### 4.1 基本流程
存在3种特殊情况
1. 所有样本在同一类别  无须划分
2. 当前属性为空 或所有属性相同 无需划分 用当前父节点最多的样本
3. 当前节点包括样本数为空 不能划分 用当前福节点最多样本

#### 4.2 划分选择
信息熵: Ent(D) = sum(-pk \* log2 pk)  
信息增益: 判断是否要继续划分  Gain(D,a) = Ent(D) - sum(Dv/D * Ent(Dv)) Dv 是d的子类  
信息增益越大 那么用该属性作为划分选择效果最好  
ID3 就是用信息增益来作为划分属性

---

由于信息增益对取值数目较多的属性有所偏好  比如 一个属性分成17个 每个1个样本 那么它信息增益很大 但是不具有泛化能力 故信息增益在一定条件下不具有意义 故引入了增益率   
增益率: Gain_ratio(D,a) = Gain(D,a)/IV(a)  IV(a) = -sum(Dv/D * log2 (Dv/D))   
C4.5的算法就是用增益率  

---

基尼指数:Gini(D) = sum(sum(pk*pk')) = 1-sum(pk*pk)    (k'!=k)
基尼指数代表随机抽取两个样本  不一样的概率
Gini_index(D,a) = sum(Dv/D * Gini(Dv)) 选择基尼指数最小的
CART决策树用基尼指数

#### 4.3 剪枝处理
剪枝是防止过拟合 分为预剪枝和后剪枝
- 预剪枝: 在划分前判断是否增加了准确率  若不增加  那么就不继续进行新的划分 这能减少过拟合的风险  但是由于本质是放置分支展开  会有欠拟合的风险  
- 后剪枝: 从底部开始  判断将节点转为叶节点 看是否会增加正确率  
  欠拟合风险小  泛化能力由于预剪枝

#### 4.4 连续与缺失值
- 连续值处理 
  主要采用二分法 也是C4.5的算法中用的机制  
  将划分点设置成两个值的中点 然后分成左边和右边 可以多次分类
- 缺失值处理  
  将之前的Gain 和 Ent 公式改变   
  Gain(D,a) = p * Gain(D',a)  p= sum(D'/D) D'代表没有缺失的  
  Ent(D') = -sum(pk' * log2 pk')  pk'代表无缺失样本中k类的数目  
  判断时 若已知则直接按照已知的做  若未知 则将他划分如所有子类 并将权值改为已有的比例 最后可以看到概率

#### 4.5 多变量决策树
之前的决策树在判断的时候只有一个变量在判断  将它放入坐标系 坐标系代表变量 那么 都是平行坐标系的直线
多变量就是 将平行坐标系变成斜线 判断时候多个变量联合判断

# chapter 5 神经网络
#### 5.1 神经元模型
数学上 神经元模型是多个y = f(sum(wx-theta)) 嵌套代入得到的  

#### 5.2 感知器与多层网络
一层功能神经元都智能左线性可分 否则会出现震荡  
线性可分: 存在一个线性超空间将他分开  
层数:不包含输入层 包含输出和隐藏层

#### 5.3 误差逆传播算法
BP 误差逆传播算法 反向传播算法  
每层的迭代步长可以不相等  
主要数学基础是梯度下降和求导的链式法则  
权值修改和连接两边的值以及上一步的改变的情况有关
标准BP算法: 每次只针对一个样例进行改变  容易出现抵消  
累计BP算法: 对一组样例进行训练 累计误差  
解决过拟合:  
1. 早停 当出现训练集误差降低但是验证集误差升高则停止训练
2. 正则化 代价增加个权值的平方和

#### 5.4 局部最小和全局最小
局部最小不一定是全局最小  由于梯度下降算法很容易到达局部最小而并不能到达全局最小  
解决方法:  
1. 用多个初始值进行训练  取最优
2. 模拟退火  每次以一定概率允许更差的结果
3. 随机梯度下降  在计算梯度的时候加入随机值

#### 5.5 其他常见神经网络
1. RBF网络  径向基网络  
  通常是单隐藏层 足够多神经元能逼近任意连续函数  
  f(x) = sum(wi * p(x,ci))  p(x,ci) = e<sup>-beta||x-ci||<sup>2</sup></sup>
  两部进行训练  1. 通过随机采样或者聚类找到神经元中心 2. 用BP确定w和beta  
  作用是拟合函数
2. ART 自适应谐振理论    
  竞争学习  无监督学习  
  优点: 能够增量学习 在线学习  
  先竞争 竞争方式是判断输入向量和每个神经元之间的向量距离 获得获胜神经元 获胜神经元向其他神经元发送信号 抑制他们激活 若相似度大于某个阈值 那么可以归为一类 若小鱼 则新增一个神经元 将向量设置为当前输入
3. SOM网络 自组织映射网络  
无监督学习  将高维数据映射到低维空间  
接受样本后 先计算与自身权向量距离 最近的神经元获胜 获胜的神经元与其临近的权向量调整 使其与当前输入样本的距离缩小 直到收殓
4. 级联相关网络  
一开始只有输入层和输出层 之后增加节点 新增节点与输入端连接权值为冻结固定  
训练速度块  容易陷入过拟合

5. Elman网络  
递归神经网络 t时刻的状态不光和它的输入有关 还和t-1时刻的状态有关  
Elman网络的隐藏层的输出会被反馈回来

6. Boltzmann机  
分为受限和非受限 受限是值同层内没有连接  
分为隐层和显层  目的是最小化能量函数

#### 5.6 深度学习
有两种主要的思想  
1. 预训练+微调 先进行逐层训练 再合在一起对网络微调 DBN
2. 权共享 设置一组神经元集合 这一组中每组的权值相同  CNN

# Chapter 6 支持向量机
[一篇很好的博客](http://blog.csdn.net/macyang/article/details/38782399/)
#### 6.1 间隔和支持向量
支持向量机做的是分类的任务  目的是用一个超平面将两类进行分类 导致 两类的间隔最大  
SVM基本型:   
min 1/2||w||<sup>2</sup>  
s.t. yi(W<sup>T</sup>xi+b) >=1
之后所讲目的是更方便的解得这个基本型

#### 6.2 对偶问题
主要用到方法是先用拉格朗日乘法变成无制约条件下的极值求解 在通过将其转化为对有问题变成求解ai 再通过求解的ai 转化为 w 和b  
对偶求解用到KKT条件  
对偶求解后是一个二次规划的问题  用到了SMO  
SMO是每次固定ai和aj以外的所有参数 ai和aj用启发式思想找出

#### 6.3 核函数
核函数的目的是将低位无法线性划分的映射到高维进行线性区分  而直接映射会导致纬度爆炸 计算复杂度过高   从而引入了核函数 用核函数来模拟高维的内积 从而直接得到结果   
核函数要求是他对仍以数据得到的核矩阵是半正定的  
常见的核函数:
1. 线性核  2. 多项式核  3. 高斯核  4. 拉普拉斯核  5. sigmoid核

#### 6.4 软间隔和正则化 
由于支持向量机对噪点会受很大的影响 且容易过拟合 故可以使用软间隔  允许部分出现错误  
方法是引入损失函数 并将原本基本型中的代价函数进行改变  
引入的损失函数为hinge瞬时  
结构风险和经验风险: 原本的代价函数相当于是结构风险  维持某种性质  加入的正则化就更像经验风险 为了训练数据的契合度

#### 6.5 支持向量回归 
之前考虑的是分类算法 现在开始考虑正对连续的回归问题
我们允许在一定间隔内不计算瞬时同时希望间隔最小  
即将之前的对偶问题进行进一步改变  

#### 6.6 核方法
讲核函数用到其他算法中 同样是将低维度的映射到高纬度的  书上讲了KLDA
ge
# Chapter 7 贝叶斯分类器
#### 7.1 贝叶斯决策论
根据先验概率推测后验概率主要有两种 一种是判别式模型 主要有决策树 BP神经网络 SVM 另一种是生成式模型 也就是这里的贝叶斯模型 根据联合概率建模
#### 7.3 朴素贝叶斯分类器
假设 属性条件独立性假设: 所有属性相互独立  
可以简单计算得到离散的先验概率 而对于连续值则可以计算均值方差 用正态分布  
拉普拉斯修正: 可能训练集中某些属性未出现 将所有的概率进行一次变化 分母加上N 分子加上1 使其不出现0 
#### 7.4 半朴素贝叶斯分类器
进行独依赖估计: 每个属性最多仅依赖一个其他属性  
做法主要有以下3种:
1. SPODE 设计一个超父 所有属性依赖它
2. TAN 先构成完全图 计算仍以两个属性的 条件互信息 构建最大生成树
3. AODE 尝试将每个属性作为超父进行构建SPODE并进行集成

#### 7.5 贝叶斯网
同样是利用联合分布 属性间的依赖更为复杂  
结构上分为三种 1. 同父结构 2. V型结构  3. 顺序结构   
判断独立性: 有向分离: 
1. 在V结构父节点上加无向边 
2. 有向图改无向图  

如果能通过去除一些点 使其变成两个联通分支 那么就是在去处的那些点下条件独立

在学习上 通过定义评分函数找到合适的贝叶斯网 
在推断上 使用吉布斯采样 通过随机采样得到后验概率
#### 7.6 EM算法
训练的时候会存在一些变量的值无法被观测到  也就是隐变量  这里就需要用EM算法  
E 根据模型参数推断隐变量  
M 根据变量和隐变量进行极大似然估计 得到参数
EM两步交替直到收敛

# chapter 8 集成学习
#### 8.1 个体与集成
集成学习是用多个学习器来完成学习任务  
如果只包含同种类型的个体学习器 则称这种集成是同质的 个体学习器叫做基学习器 学习算法叫做基学习算法  
如果由不同算法进行集成 那么不存在基学习算法  个体学习器称为组件学习器或个体学习器  
弱学习器: 泛化能力略高于随机猜测的学习器  
在基学习器误差相互独立的情况下 集成的错误率将指数级下降 最终趋向于0  
集成学习可以分成两类:
1. 个体学习器之间存在强依赖关系 必须串行生成序列化方法 Boosting
2. 个体学习器之间没有强依赖关系 可以并行运行  Bagging 和随机森林

#### 8.2 Boosting
该算法的思想是用多个若分类器进行分类 如果前一个分类器分类正确 那么减少后一个分类器的权值 若错误 增加后一个分类器的权值  若正确率低于0.5 则停止 每个权重为0.5ln(1-e/e) e为误差率  
如果不能很好的做到修改权值 择可以进行从采样
该算法主要关注降低偏差  

#### 8.3 Bagging与随机森林
Bagging算法的思想是对样本进行多次采样 基于每个采样得到一个基学习器  集成结果是在多个学习器的结果上进行投票  
该算法主要降低方差  

---

随机森林算法
在决策树寻找最优属性进行划分的这步中 加入随机控制k 将原本从所有可能进行划分改为从随机选择的k个内进行划分

#### 8.4 结合策略
集成学习带来的三个好处
1. 增加泛化性
2. 减少陷入局部最小值的风险
3. 更有可能在要求的假设空间中  

结合策略有 平均法 加权平均法 投票法(绝对多数(若为得到绝对多数则拒绝预测) 相对多数 加权) 学习法  
学习法是需要两个学习器 先将初级学习器的输出当做次级学习器的特征输入

#### 8.5 多样性
1. 误差-分歧分解  目的是让学习器“好而不同” 公式是每个学习器与集成学习器的平均平方误差-集成学习器与真值的平均平方误差
2. 多样性度量 判断分类器两类的不相似性 有4个常用的通缉量
为增强多样性 可以对数据样本  输入属性  输出 算法进行加入随机性
